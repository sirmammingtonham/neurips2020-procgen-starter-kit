{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter script found. Skipping steps\n"
     ]
    }
   ],
   "source": [
    "!bash ./onCreate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When running this notebook on SageMaker Studio, you should make sure the 'SageMaker JumpStart Tensorflow 1.0' image/kernel is used. You can run run all cells at once or step through the notebook.\n",
    "# Amazon SageMaker Notebook for ProcGen Starter Kit with Single Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stack_outputs.json\") as f:\n",
    "    sagemaker_config = json.load(f)\n",
    "sagemaker_config['GPUTrainingInstance'] = 'ml.g4dn.4xlarge'\n",
    "sagemaker_config['CPUTrainingInstance'] = 'ml.c5.4xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-soln-drl-procgen-workpls-neurips-2020-workplsoa2/\n"
     ]
    }
   ],
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_config[\"S3Bucket\"]\n",
    "\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::399362199941:role/sagemaker-soln-drl-procgen-workpls-nb-role\n"
     ]
    }
   ],
   "source": [
    "job_name_prefix = 'sm-ray-procgen'\n",
    "\n",
    "role = sagemaker_config[\"SageMakerIamRoleArn\"]\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure training instance type and computational resources\n",
    "\n",
    "By default (`local_mode=False`) launch a separate instance for training and debug using the AWS CloudWatch to monitor the logs for the training instance. \n",
    "If you want to train on the same instance as your notebook for quick debugging, then set `local_mode=True`. \n",
    "\n",
    "The recommended instances include with cost per hour as of September, 1, 2020 are:\n",
    "* `ml.c5.4xlarge` $0.952 per hour (16 vCPU)\n",
    "\n",
    "* `ml.g4dn.4xlarge` $1.686 per hour (1 GPU, 16 vCPU)\n",
    "\n",
    "* `ml.p3.2xlarge` $4.284 per hour (1 GPU, 8 vCPU)\n",
    "\n",
    "After you choose your instance type, make sure the edit the resources in [`source\\train-sagemaker.py`](./source/train-sagemaker.py). For example, with `ml.p3.2xlarge`, you have 1 GPU and 8 vCPUs. The corresponding resources in [`source\\train-sagemaker.py`](./source/train-sagemaker.py) should be set as for `ray` as `\n",
    "\n",
    "```\n",
    "    def _get_ray_config(self):\n",
    "        return {\n",
    "            \"ray_num_cpus\": 8, # adjust based on selected instance type\n",
    "            \"ray_num_gpus\": 1,\n",
    "            \"eager\": False,\n",
    "             \"v\": True, # requried for CW to catch the progress\n",
    "        }\n",
    "``` \n",
    "and for `rrlib` need to use 1 vCPU for driver (\"num_workers\": 7) and 1 GPU (\"num_gpus\": 1) for policy training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change local_mode to True if you want to do local training within this Notebook instance\n",
    "# Otherwise, we'll spin-up a SageMaker training instance to handle the training\n",
    "\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    instance_type = sagemaker_config[\"GPUTrainingInstance\"]\n",
    "    \n",
    "# If training locally, do some Docker housekeeping..\n",
    "if local_mode:\n",
    "    !/bin/bash source/common/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AwsAccountId': '399362199941',\n",
       " 'AwsRegion': 'us-west-2',\n",
       " 'S3Bucket': 'sagemaker-soln-drl-procgen-workpls-neurips-2020-workplsoa2',\n",
       " 'SageMakerIamRoleArn': 'arn:aws:iam::399362199941:role/sagemaker-soln-drl-procgen-workpls-nb-role',\n",
       " 'ECRImageBaseName': 'sagemaker-soln-drl-procgen-workpls-image',\n",
       " 'SolutionCodeBuildProject': 'sagemaker-soln-drl-procgen-workpls-image-container-build',\n",
       " 'CPUTrainingInstance': 'ml.c5.4xlarge',\n",
       " 'GPUTrainingInstance': 'ml.g4dn.4xlarge'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the framework you want to use\n",
    "\n",
    "Set `framework` to `\"tf\"` or `\"torch\"` for tensorflow or pytorch respectively.\n",
    "\n",
    "You will also have to edit your entry point i.e., [`train-sagemaker.py`](./source/train-sagemaker.py) with the configuration parameter `\"use_pytorch\"` to match the framework that you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = \"tf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the training code\n",
    "\n",
    "The training code is written in the file [`train-sagemaker.py`]((./source/train-sagemaker.py)) which is uploaded in the /source directory.\n",
    "\n",
    "#### *Warning: Confirm that the GPU and CPU resources are configured correctly for your instance type as described above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize source/train-sagemaker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "If you are using local mode, the training will run on the notebook instance. \n",
    "\n",
    "When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs.\n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded.\n",
    "2. Specify the entry point as the training code\n",
    "3. Specify the custom image to be used for the training environment.\n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name.\n",
    "5. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Choose](https://github.com/aws/sagemaker-rl-container#rl-images-provided-by-sagemaker) which docker image to use based on the instance type.* \n",
    "For this notebook, it has to be a container with Ray 0.8.5 and TensorFlow 2.1.0 to be consistent with the AICrowd ProcGen starter kit. \n",
    "\n",
    "If you prefer to use PyTorch, it is recommended to update your notebook kernel to `conda_pytorch_p36`. You would need to substitute for the corresponding container listed on Amazon SageMaker Reinforcement Learning documentation. In addition, you will need to ensure your starter kit is modified to train using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'462105765813.dkr.ecr.us-west-2.amazonaws.com/sagemaker-rl-ray-container:ray-0.8.5-tf-gpu-py36'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_or_gpu = 'gpu' if instance_type.startswith(('ml.p', 'ml.g')) else 'cpu'\n",
    "aws_region = boto3.Session().region_name\n",
    "\n",
    "# Use Tensorflow 2 by default\n",
    "custom_image_name = \"462105765813.dkr.ecr.{}.amazonaws.com/sagemaker-rl-ray-container:ray-0.8.5-{}-{}-py36\".format(aws_region, framework, cpu_or_gpu)\n",
    "custom_image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define metrics to be displayed in the logs. The challenge has requirements on the number of steps and uses mean episode reward to rank various solutions. For details, refer to the AICrowd challange website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions =  [\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episodes_total', 'Regex': 'episodes_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'num_steps_trained', 'Regex': 'num_steps_trained: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'timesteps_total', 'Regex': 'timesteps_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'episode_reward_max', 'Regex': 'episode_reward_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_mean', 'Regex': 'episode_reward_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_min', 'Regex': 'episode_reward_min: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the RL estimator\n",
    "\n",
    "There are 16 environments to choose from. You can run the RL estimator on multiple environments by proving a list of environments as well. The RL estimator will start the training job. This will take longer compared to the above cells, be patient. You can monitor the status of your training job from the console as well, go to Amazon SageMaker > Training jobs. The most recent job will be at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which procgen environments to run in `envs_to_run`\n",
    "'''\n",
    "envs_to_run = [\"coinrun\", \"bigfish\", \"bossfight\", \"caveflyer\",\n",
    "               \"chaser\", \"climber\", \"dodgeball\", \"maze\",\n",
    "               \"fruitbot\", \"heist\", \"jumper\", \"leaper\",\n",
    "               \"miner\", \"ninja\", \"plunder\", \"starpilot\"]\n",
    "'''\n",
    "\n",
    "# envs_to_run = [\"coinrun\", \"bigfish\", \"bossfight\"]\n",
    "envs_to_run = [\"caveflyer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-ray-procgen-caveflyer-2020-12-17-11-17-13-798\n"
     ]
    }
   ],
   "source": [
    "for env in envs_to_run:\n",
    "    estimator = RLEstimator(entry_point=\"train-sagemaker.py\",\n",
    "                            source_dir='source',\n",
    "                            dependencies=[\"source/utils\", \"source/common/\", \"neurips2020-procgen-starter-kit/\"],\n",
    "                            image_uri=custom_image_name,\n",
    "                            role=role,\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=1,\n",
    "                            output_path=s3_output_path,\n",
    "                            base_job_name=job_name_prefix + \"-\" + env,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            debugger_hook_config=False,\n",
    "                            hyperparameters={\n",
    "                                #\"rl.training.upload_dir\": s3_output_path,\n",
    "                                \"rl.training.config.env_config.env_name\": env,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    estimator.fit(wait=False)\n",
    "    \n",
    "    print(estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WAAAITTTTT... not more than 2 hours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize algorithm metrics for training\n",
    "\n",
    "There are several options to visualize algorithm metrics. A detailed blog can be found [here](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/).\n",
    "\n",
    "\n",
    "Option 1 (Amazon CloudWatch): You can go to the [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/) metrics dashboard from your account to monitor and visualize the algorithm metrics as well as track the GPU and CPU usage. The training jobs details page has a direct link to the Amazon CloudWatch metrics dashboard for the metrics emitted by the training algorithm.\n",
    "\n",
    "Option 2 (Amazon SageMaker Python SDK API): You can also visualize the metrics inline in your Amazon SageMaker Jupyter notebooks using the Amazon SageMaker Python SDK APIs. Please, refer to the section titled *Visualize algorithm metrics for training* in `train.ipynb`.\n",
    "\n",
    "Option 3 (Tensorboard): You can also use Ray Tune's integrated Tensorboard by specifying the output directory of your results. It is recommended to set `upload_dir` to a Amazon S3 URI and Tune will automatically sync every 5 miniutes. You can thus visualize your experiment by running the following command on your local laptop:\n",
    "\n",
    "`\n",
    "$AWS_REGION=your-aws-region tensorboard --logdir s3://destination_s3_path --host localhost --port 6006\n",
    "`\n",
    "\n",
    "Check out `train-homo-distributed-cpu.ipynb` for an example of setting `upload_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Plot metrics using Amazon SageMaker Python SDK API\n",
    "\n",
    "You need to wait for the training job to allocate computational resources before viewing the logs. \n",
    "\n",
    "*Note: If you get a warning that the logs do not exist, wait for a few minutes and re-run the cell.*\n",
    "\n",
    "*Note 2: If you are getting an import error from Tensorflow, open a terminal and type `source activate tensorflow2_p36`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For usage, refer to https://sagemaker.readthedocs.io/en/stable/api/training/analytics.html#\n",
    "from sagemaker.analytics import TrainingJobAnalytics\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from source.utils.inference import get_latest_sagemaker_training_job\n",
    "\n",
    "# Get last training job_names\n",
    "eval_training_jobs = [get_latest_sagemaker_training_job(name_contains=\"{}-{}\".format(\n",
    "    job_name_prefix, env)) for env in envs_to_run]\n",
    "\n",
    "for training_job_name, env in zip(eval_training_jobs, envs_to_run):\n",
    "    metric_names = ['episode_reward_mean', 'timesteps_total']\n",
    "\n",
    "    # download the metrics on cloudwatch\n",
    "    metrics_dataframe = TrainingJobAnalytics(training_job_name=training_job_name, metric_names=metric_names).dataframe()\n",
    "\n",
    "    # pivot to get the metrics\n",
    "    metrics_dataframe= metrics_dataframe.pivot(index='timestamp', columns='metric_name', values='value')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = metrics_dataframe.plot(kind='line', figsize=(12, 5), x='timesteps_total', y='episode_reward_mean', style='b.', legend=False)\n",
    "    ax.set_ylabel('Episode Reward Mean')\n",
    "    ax.set_xlabel('Timesteps')\n",
    "    ax.set_title(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigations\n",
    "You can look through the other notebooks if you want to:\n",
    "- [(here) Single instance training with cpu/gpu instance](./1_train.ipynb)\n",
    "- [Distributed training with cpu instances](./2_train-homo-distributed-cpu.ipynb)\n",
    "- [Distributed training with gpu instances](./3_train-homo-distributed-gpu.ipynb)\n",
    "- [Distributed training with both cpu and gpu instances](./4_train-hetero-distributed.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
